{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-20T20:21:58.750154Z",
     "iopub.status.busy": "2025-10-20T20:21:58.749529Z",
     "iopub.status.idle": "2025-10-20T20:22:01.854242Z",
     "shell.execute_reply": "2025-10-20T20:22:01.853419Z",
     "shell.execute_reply.started": "2025-10-20T20:21:58.750132Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/foundations-of-science-book/pg39713.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T20:08:38.154568Z",
     "iopub.status.busy": "2025-10-20T20:08:38.154107Z",
     "iopub.status.idle": "2025-10-20T20:09:27.493683Z",
     "shell.execute_reply": "2025-10-20T20:09:27.492800Z",
     "shell.execute_reply.started": "2025-10-20T20:08:38.154550Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%#####                                                     29.7%#######################                                     52.3%########################                                 57.3%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "import subprocess\n",
    "process = subprocess.Popen(\"ollama serve\", shell=True) #runs on a different thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T20:09:27.495243Z",
     "iopub.status.busy": "2025-10-20T20:09:27.494935Z",
     "iopub.status.idle": "2025-10-20T20:09:59.627948Z",
     "shell.execute_reply": "2025-10-20T20:09:59.627127Z",
     "shell.execute_reply.started": "2025-10-20T20:09:27.495219Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\n",
      "Your new public key is: \n",
      "\n",
      "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBGq1ksSYtGNMAtZSHUfaKGANP5qmCMjDALy7InCz0r/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-10-20T20:09:27.518Z level=INFO source=routes.go:1511 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2025-10-20T20:09:27.519Z level=INFO source=images.go:522 msg=\"total blobs: 0\"\n",
      "time=2025-10-20T20:09:27.519Z level=INFO source=images.go:529 msg=\"total unused blobs removed: 0\"\n",
      "time=2025-10-20T20:09:27.520Z level=INFO source=routes.go:1564 msg=\"Listening on 127.0.0.1:11434 (version 0.12.6)\"\n",
      "time=2025-10-20T20:09:27.520Z level=INFO source=runner.go:80 msg=\"discovering available GPUs...\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/10/20 - 20:09:27 | 200 |      78.731µs |       127.0.0.1 | HEAD     \"/\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-10-20T20:09:27.963Z level=INFO source=types.go:112 msg=\"inference compute\" id=GPU-1df0b5e8-ab5e-0f0b-4f4f-216233b7cc9e library=CUDA compute=6.0 name=CUDA0 description=\"Tesla P100-PCIE-16GB\" libdirs=ollama,cuda_v12 driver=12.6 pci_id=00:04.0 type=discrete total=\"16.0 GiB\" available=\"15.9 GiB\"\n",
      "time=2025-10-20T20:09:27.963Z level=INFO source=routes.go:1605 msg=\"entering low vram mode\" \"total vram\"=\"16.0 GiB\" threshold=\"20.0 GiB\"\n",
      "time=2025-10-20T20:09:29.465Z level=INFO source=download.go:177 msg=\"downloading 3d0b790534fe in 14 100 MB part(s)\"\n",
      "time=2025-10-20T20:09:35.076Z level=INFO source=download.go:177 msg=\"downloading ae370d884f10 in 1 1.7 KB part(s)\"\n",
      "time=2025-10-20T20:09:36.703Z level=INFO source=download.go:177 msg=\"downloading d18a5cc71b84 in 1 11 KB part(s)\"\n",
      "time=2025-10-20T20:09:41.448Z level=INFO source=download.go:177 msg=\"downloading cff3f395ef37 in 1 120 B part(s)\"\n",
      "time=2025-10-20T20:09:43.042Z level=INFO source=download.go:177 msg=\"downloading 517ccaff02fe in 1 487 B part(s)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/10/20 - 20:09:48 | 200 |  20.42740552s |       127.0.0.1 | POST     \"/api/pull\"\n",
      "[GIN] 2025/10/20 - 20:09:48 | 200 |      26.047µs |       127.0.0.1 | HEAD     \"/\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-10-20T20:09:49.749Z level=INFO source=download.go:177 msg=\"downloading 06507c7b4268 in 7 100 MB part(s)\"\n",
      "time=2025-10-20T20:09:56.357Z level=INFO source=download.go:177 msg=\"downloading 9202febed9e2 in 1 266 B part(s)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/10/20 - 20:09:59 | 200 | 10.974847071s |       127.0.0.1 | POST     \"/api/pull\"\n"
     ]
    }
   ],
   "source": [
    "!ollama pull qwen3:1.7b > /dev/null 2>&1\n",
    "!ollama pull qwen3-embedding:0.6b > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T20:09:59.629253Z",
     "iopub.status.busy": "2025-10-20T20:09:59.628993Z",
     "iopub.status.idle": "2025-10-20T20:09:59.792536Z",
     "shell.execute_reply": "2025-10-20T20:09:59.791569Z",
     "shell.execute_reply.started": "2025-10-20T20:09:59.629221Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/10/20 - 20:09:59 | 200 |      32.579µs |       127.0.0.1 | HEAD     \"/\"\n",
      "[GIN] 2025/10/20 - 20:09:59 | 200 |     830.822µs |       127.0.0.1 | GET      \"/api/tags\"\n",
      "NAME                    ID              SIZE      MODIFIED               \n",
      "qwen3-embedding:0.6b    ac6da0dfba84    639 MB    Less than a second ago    \n",
      "qwen3:1.7b              8f68893c685c    1.4 GB    11 seconds ago            \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Qwen 3 and FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T20:10:15.847315Z",
     "iopub.status.busy": "2025-10-20T20:10:15.846750Z",
     "iopub.status.idle": "2025-10-20T20:10:16.375598Z",
     "shell.execute_reply": "2025-10-20T20:10:16.374849Z",
     "shell.execute_reply.started": "2025-10-20T20:10:15.847289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "embedder = OllamaEmbeddings(model=\"qwen3-embedding:0.6b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T20:23:04.775530Z",
     "iopub.status.busy": "2025-10-20T20:23:04.774102Z",
     "iopub.status.idle": "2025-10-20T20:23:27.921234Z",
     "shell.execute_reply": "2025-10-20T20:23:27.920285Z",
     "shell.execute_reply.started": "2025-10-20T20:23:04.775502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8317ea27ca4948905f17eeb157ff46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc3307d78e245dc84e1e044ada03b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.38G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b463eef224e4338919f7743734bc92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd94443cb1743cdbf90d526fcb0640b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586ef9dec1174ebaae1c44ef55d1471a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c200bdbed8c4bb4b4c2a538923c1544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800def37ed424dfc943a9f9127c0955c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "reranker = CrossEncoder(\"tomaarsen/Qwen3-Reranker-0.6B-seq-cls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents into FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T20:11:21.356241Z",
     "iopub.status.busy": "2025-10-20T20:11:21.355526Z",
     "iopub.status.idle": "2025-10-20T20:11:21.419384Z",
     "shell.execute_reply": "2025-10-20T20:11:21.418578Z",
     "shell.execute_reply.started": "2025-10-20T20:11:21.356217Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loader = TextLoader(\n",
    "    file_path=\"/kaggle/input/foundations-of-science-book/pg39713.txt\"\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T20:11:23.037244Z",
     "iopub.status.busy": "2025-10-20T20:11:23.036727Z",
     "iopub.status.idle": "2025-10-20T20:11:23.061602Z",
     "shell.execute_reply": "2025-10-20T20:11:23.061036Z",
     "shell.execute_reply.started": "2025-10-20T20:11:23.037223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T20:11:24.893856Z",
     "iopub.status.busy": "2025-10-20T20:11:24.893252Z",
     "iopub.status.idle": "2025-10-20T20:12:44.671209Z",
     "shell.execute_reply": "2025-10-20T20:12:44.670574Z",
     "shell.execute_reply.started": "2025-10-20T20:11:24.893837Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-10-20T20:11:24.983Z level=WARN source=types.go:753 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-10-20T20:11:24.983Z level=WARN source=types.go:753 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-10-20T20:11:24.983Z level=WARN source=types.go:753 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-10-20T20:11:24.983Z level=WARN source=types.go:753 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-10-20T20:11:25.302Z level=WARN source=server.go:209 msg=\"flash attention enabled but not supported by model\"\n",
      "time=2025-10-20T20:11:25.302Z level=INFO source=server.go:400 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --model /root/.ollama/models/blobs/sha256-06507c7b42688469c4e7298b0a1e16deff06caf291cf0a5b278c308249c3e439 --port 39905\"\n",
      "time=2025-10-20T20:11:25.303Z level=INFO source=server.go:676 msg=\"loading model\" \"model layers\"=29 requested=-1\n",
      "time=2025-10-20T20:11:25.303Z level=INFO source=server.go:682 msg=\"system memory\" total=\"31.4 GiB\" free=\"27.0 GiB\" free_swap=\"0 B\"\n",
      "time=2025-10-20T20:11:25.303Z level=INFO source=server.go:690 msg=\"gpu memory\" id=GPU-1df0b5e8-ab5e-0f0b-4f4f-216233b7cc9e library=CUDA available=\"13.0 GiB\" free=\"13.4 GiB\" minimum=\"457.0 MiB\" overhead=\"0 B\"\n",
      "time=2025-10-20T20:11:25.320Z level=INFO source=runner.go:1332 msg=\"starting ollama engine\"\n",
      "time=2025-10-20T20:11:25.324Z level=INFO source=runner.go:1367 msg=\"Server listening on 127.0.0.1:39905\"\n",
      "time=2025-10-20T20:11:25.326Z level=INFO source=runner.go:1205 msg=load request=\"{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:2 GPULayers:29[ID:GPU-1df0b5e8-ab5e-0f0b-4f4f-216233b7cc9e Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-10-20T20:11:25.364Z level=INFO source=ggml.go:134 msg=\"\" architecture=qwen3 file_type=Q8_0 name=\"Qwen3 Embedding 0.6b\" description=\"\" num_tensors=310 num_key_values=37\n",
      "load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-skylakex.so\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes, ID: GPU-1df0b5e8-ab5e-0f0b-4f4f-216233b7cc9e\n",
      "load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v12/libggml-cuda.so\n",
      "time=2025-10-20T20:11:25.464Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
      "time=2025-10-20T20:11:25.607Z level=INFO source=runner.go:1205 msg=load request=\"{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:2 GPULayers:29[ID:GPU-1df0b5e8-ab5e-0f0b-4f4f-216233b7cc9e Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-10-20T20:11:25.662Z level=INFO source=runner.go:1205 msg=load request=\"{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:2 GPULayers:29[ID:GPU-1df0b5e8-ab5e-0f0b-4f4f-216233b7cc9e Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-10-20T20:11:25.662Z level=INFO source=ggml.go:480 msg=\"offloading 28 repeating layers to GPU\"\n",
      "time=2025-10-20T20:11:25.662Z level=INFO source=ggml.go:487 msg=\"offloading output layer to GPU\"\n",
      "time=2025-10-20T20:11:25.662Z level=INFO source=ggml.go:492 msg=\"offloaded 29/29 layers to GPU\"\n",
      "time=2025-10-20T20:11:25.662Z level=INFO source=device.go:206 msg=\"model weights\" device=CUDA0 size=\"603.9 MiB\"\n",
      "time=2025-10-20T20:11:25.662Z level=INFO source=device.go:211 msg=\"model weights\" device=CPU size=\"157.4 MiB\"\n",
      "time=2025-10-20T20:11:25.662Z level=INFO source=device.go:217 msg=\"kv cache\" device=CUDA0 size=\"448.0 MiB\"\n",
      "time=2025-10-20T20:11:25.662Z level=INFO source=device.go:228 msg=\"compute graph\" device=CUDA0 size=\"216.0 MiB\"\n",
      "time=2025-10-20T20:11:25.662Z level=INFO source=device.go:233 msg=\"compute graph\" device=CPU size=\"2.0 MiB\"\n",
      "time=2025-10-20T20:11:25.662Z level=INFO source=device.go:238 msg=\"total memory\" size=\"1.4 GiB\"\n",
      "time=2025-10-20T20:11:25.662Z level=INFO source=sched.go:482 msg=\"loaded runners\" count=1\n",
      "time=2025-10-20T20:11:25.662Z level=INFO source=server.go:1272 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-10-20T20:11:25.663Z level=INFO source=server.go:1306 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "time=2025-10-20T20:11:26.166Z level=INFO source=server.go:1310 msg=\"llama runner started in 0.86 seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/10/20 - 20:12:44 | 200 |         1m19s |       127.0.0.1 | POST     \"/api/embed\"\n"
     ]
    }
   ],
   "source": [
    "vector_store = FAISS.from_documents(all_splits, embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T20:12:44.672556Z",
     "iopub.status.busy": "2025-10-20T20:12:44.672298Z",
     "iopub.status.idle": "2025-10-20T20:12:45.009898Z",
     "shell.execute_reply": "2025-10-20T20:12:45.009130Z",
     "shell.execute_reply.started": "2025-10-20T20:12:44.672532Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create QA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-20T19:54:24.827365Z",
     "iopub.status.idle": "2025-10-20T19:54:24.827665Z",
     "shell.execute_reply": "2025-10-20T19:54:24.827523Z",
     "shell.execute_reply.started": "2025-10-20T19:54:24.827510Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-20T19:54:24.828622Z",
     "iopub.status.idle": "2025-10-20T19:54:24.828918Z",
     "shell.execute_reply": "2025-10-20T19:54:24.828782Z",
     "shell.execute_reply.started": "2025-10-20T19:54:24.828769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "query = \"Can you explain thermodynamics?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-20T19:54:24.830177Z",
     "iopub.status.idle": "2025-10-20T19:54:24.830435Z",
     "shell.execute_reply": "2025-10-20T19:54:24.830298Z",
     "shell.execute_reply.started": "2025-10-20T19:54:24.830289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "query = \"Who is the author The Foundations of Science?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA chain with reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T20:43:56.178993Z",
     "iopub.status.busy": "2025-10-20T20:43:56.178548Z",
     "iopub.status.idle": "2025-10-20T20:43:56.189523Z",
     "shell.execute_reply": "2025-10-20T20:43:56.188985Z",
     "shell.execute_reply.started": "2025-10-20T20:43:56.178968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "from pydantic import PrivateAttr\n",
    "\n",
    "DEFAULT_TASK = (\n",
    "    \"Given a question about the book, rank the passages by how useful they are for answering the question. \"\n",
    "    \"Only consider content from the book that directly supports a correct and concise answer.\"\n",
    ")\n",
    "\n",
    "def rerank_documents(query, docs, task=DEFAULT_TASK, top_k=3):\n",
    "    def format_query(q):\n",
    "        return (\n",
    "            f\"<|im_start|>system\\n\"\n",
    "            f\"Judge whether the Document meets the requirements based on the Query and the Instruct provided. \"\n",
    "            f\"Note that the answer can only be \\\"yes\\\" or \\\"no\\\".\"\n",
    "            f\"<|im_end|>\\n<|im_start|>user\\n\"\n",
    "            f\"<Instruct>: {task}\\n<Query>: {q}\\n\"\n",
    "        )\n",
    "\n",
    "    def format_doc(d):\n",
    "        return f\"<Document>: {d}<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "\n",
    "    texts = [d.page_content if isinstance(d, Document) else d for d in docs]\n",
    "    pairs = [(format_query(query), format_doc(t)) for t in texts]\n",
    "    scores = reranker.predict(pairs)\n",
    "    ranked_idx = np.argsort(scores)[::-1]\n",
    "    return [docs[i] for i in ranked_idx[:top_k]]\n",
    "\n",
    "class RerankingRetriever(BaseRetriever):\n",
    "    # use these as private attributes instead of the ones of the base class\n",
    "    _base_retriever: BaseRetriever = PrivateAttr()\n",
    "    _top_k: int = PrivateAttr()\n",
    "\n",
    "    def __init__(self, base_retriever, top_k=3):\n",
    "        super().__init__()\n",
    "        self._base_retriever = base_retriever\n",
    "        self._top_k = top_k\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        docs = self._base_retriever.get_relevant_documents(query)\n",
    "        return rerank_documents(query, docs, top_k=self._top_k)\n",
    "\n",
    "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        docs = await self._base_retriever.aget_relevant_documents(query)\n",
    "        return rerank_documents(query, docs, top_k=self._top_k)\n",
    "\n",
    "reranking_retriever = RerankingRetriever(retriever, top_k=3)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=reranking_retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T20:43:59.162708Z",
     "iopub.status.busy": "2025-10-20T20:43:59.162196Z",
     "iopub.status.idle": "2025-10-20T20:44:05.378230Z",
     "shell.execute_reply": "2025-10-20T20:44:05.375764Z",
     "shell.execute_reply.started": "2025-10-20T20:43:59.162688Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-10-20T20:43:59.219Z level=WARN source=types.go:753 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-10-20T20:43:59.219Z level=WARN source=types.go:753 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-10-20T20:43:59.219Z level=WARN source=types.go:753 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-10-20T20:43:59.219Z level=WARN source=types.go:753 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-10-20T20:43:59.531Z level=WARN source=server.go:209 msg=\"flash attention enabled but not supported by model\"\n",
      "time=2025-10-20T20:43:59.531Z level=INFO source=server.go:400 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --model /root/.ollama/models/blobs/sha256-06507c7b42688469c4e7298b0a1e16deff06caf291cf0a5b278c308249c3e439 --port 35309\"\n",
      "time=2025-10-20T20:43:59.532Z level=INFO source=server.go:676 msg=\"loading model\" \"model layers\"=29 requested=-1\n",
      "time=2025-10-20T20:43:59.532Z level=INFO source=server.go:682 msg=\"system memory\" total=\"31.4 GiB\" free=\"25.7 GiB\" free_swap=\"0 B\"\n",
      "time=2025-10-20T20:43:59.532Z level=INFO source=server.go:690 msg=\"gpu memory\" id=GPU-1df0b5e8-ab5e-0f0b-4f4f-216233b7cc9e library=CUDA available=\"10.6 GiB\" free=\"11.1 GiB\" minimum=\"457.0 MiB\" overhead=\"0 B\"\n",
      "time=2025-10-20T20:43:59.551Z level=INFO source=runner.go:1332 msg=\"starting ollama engine\"\n",
      "time=2025-10-20T20:43:59.554Z level=INFO source=runner.go:1367 msg=\"Server listening on 127.0.0.1:35309\"\n",
      "time=2025-10-20T20:43:59.555Z level=INFO source=runner.go:1205 msg=load request=\"{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:2 GPULayers:29[ID:GPU-1df0b5e8-ab5e-0f0b-4f4f-216233b7cc9e Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-10-20T20:43:59.593Z level=INFO source=ggml.go:134 msg=\"\" architecture=qwen3 file_type=Q8_0 name=\"Qwen3 Embedding 0.6b\" description=\"\" num_tensors=310 num_key_values=37\n",
      "load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-skylakex.so\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes, ID: GPU-1df0b5e8-ab5e-0f0b-4f4f-216233b7cc9e\n",
      "load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v12/libggml-cuda.so\n",
      "time=2025-10-20T20:43:59.694Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
      "time=2025-10-20T20:43:59.837Z level=INFO source=runner.go:1205 msg=load request=\"{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:2 GPULayers:29[ID:GPU-1df0b5e8-ab5e-0f0b-4f4f-216233b7cc9e Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-10-20T20:43:59.894Z level=INFO source=runner.go:1205 msg=load request=\"{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:2 GPULayers:29[ID:GPU-1df0b5e8-ab5e-0f0b-4f4f-216233b7cc9e Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-10-20T20:43:59.894Z level=INFO source=ggml.go:480 msg=\"offloading 28 repeating layers to GPU\"\n",
      "time=2025-10-20T20:43:59.894Z level=INFO source=ggml.go:487 msg=\"offloading output layer to GPU\"\n",
      "time=2025-10-20T20:43:59.894Z level=INFO source=ggml.go:492 msg=\"offloaded 29/29 layers to GPU\"\n",
      "time=2025-10-20T20:43:59.894Z level=INFO source=device.go:206 msg=\"model weights\" device=CUDA0 size=\"603.9 MiB\"\n",
      "time=2025-10-20T20:43:59.894Z level=INFO source=device.go:211 msg=\"model weights\" device=CPU size=\"157.4 MiB\"\n",
      "time=2025-10-20T20:43:59.894Z level=INFO source=device.go:217 msg=\"kv cache\" device=CUDA0 size=\"448.0 MiB\"\n",
      "time=2025-10-20T20:43:59.894Z level=INFO source=device.go:228 msg=\"compute graph\" device=CUDA0 size=\"216.0 MiB\"\n",
      "time=2025-10-20T20:43:59.894Z level=INFO source=device.go:233 msg=\"compute graph\" device=CPU size=\"2.0 MiB\"\n",
      "time=2025-10-20T20:43:59.894Z level=INFO source=device.go:238 msg=\"total memory\" size=\"1.4 GiB\"\n",
      "time=2025-10-20T20:43:59.894Z level=INFO source=sched.go:482 msg=\"loaded runners\" count=1\n",
      "time=2025-10-20T20:43:59.894Z level=INFO source=server.go:1272 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-10-20T20:43:59.896Z level=INFO source=server.go:1306 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/10/20 - 20:44:00 | 200 |  1.316194281s |       127.0.0.1 | POST     \"/api/embed\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-10-20T20:44:00.398Z level=INFO source=server.go:1310 msg=\"llama runner started in 0.87 seconds\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5919c5f0503e48e0ba0d0dce2335bc90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 9021030400 total: 17179869184\n",
      "time=2025-10-20T20:44:01.948Z level=INFO source=sched.go:545 msg=\"updated VRAM based on existing loaded models\" gpu=GPU-1df0b5e8-ab5e-0f0b-4f4f-216233b7cc9e library=CUDA total=\"16.0 GiB\" available=\"8.4 GiB\"\n",
      "time=2025-10-20T20:44:02.029Z level=INFO source=server.go:216 msg=\"enabling flash attention\"\n",
      "time=2025-10-20T20:44:02.029Z level=INFO source=server.go:400 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --model /root/.ollama/models/blobs/sha256-3d0b790534fe4b79525fc3692950408dca41171676ed7e21db57af5c65ef6ab6 --port 45011\"\n",
      "time=2025-10-20T20:44:02.030Z level=INFO source=server.go:676 msg=\"loading model\" \"model layers\"=29 requested=-1\n",
      "time=2025-10-20T20:44:02.030Z level=INFO source=server.go:682 msg=\"system memory\" total=\"31.4 GiB\" free=\"25.2 GiB\" free_swap=\"0 B\"\n",
      "time=2025-10-20T20:44:02.030Z level=INFO source=server.go:690 msg=\"gpu memory\" id=GPU-1df0b5e8-ab5e-0f0b-4f4f-216233b7cc9e library=CUDA available=\"8.0 GiB\" free=\"8.4 GiB\" minimum=\"457.0 MiB\" overhead=\"0 B\"\n",
      "time=2025-10-20T20:44:02.048Z level=INFO source=runner.go:1332 msg=\"starting ollama engine\"\n",
      "time=2025-10-20T20:44:02.051Z level=INFO source=runner.go:1367 msg=\"Server listening on 127.0.0.1:45011\"\n",
      "time=2025-10-20T20:44:02.053Z level=INFO source=runner.go:1205 msg=load request=\"{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:2 GPULayers:29[ID:GPU-1df0b5e8-ab5e-0f0b-4f4f-216233b7cc9e Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-10-20T20:44:02.090Z level=INFO source=ggml.go:134 msg=\"\" architecture=qwen3 file_type=Q4_K_M name=\"Qwen3 1.7B\" description=\"\" num_tensors=311 num_key_values=28\n",
      "load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-skylakex.so\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes, ID: GPU-1df0b5e8-ab5e-0f0b-4f4f-216233b7cc9e\n",
      "load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v12/libggml-cuda.so\n",
      "time=2025-10-20T20:44:02.191Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,520,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
      "time=2025-10-20T20:44:02.477Z level=INFO source=runner.go:1205 msg=load request=\"{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:2 GPULayers:29[ID:GPU-1df0b5e8-ab5e-0f0b-4f4f-216233b7cc9e Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-10-20T20:44:02.544Z level=INFO source=device.go:206 msg=\"model weights\" device=CUDA0 size=\"1.1 GiB\"\n",
      "time=2025-10-20T20:44:02.544Z level=INFO source=device.go:211 msg=\"model weights\" device=CPU size=\"166.9 MiB\"\n",
      "time=2025-10-20T20:44:02.544Z level=INFO source=device.go:217 msg=\"kv cache\" device=CUDA0 size=\"448.0 MiB\"\n",
      "time=2025-10-20T20:44:02.544Z level=INFO source=device.go:228 msg=\"compute graph\" device=CUDA0 size=\"84.0 MiB\"\n",
      "time=2025-10-20T20:44:02.544Z level=INFO source=device.go:233 msg=\"compute graph\" device=CPU size=\"4.0 MiB\"\n",
      "time=2025-10-20T20:44:02.544Z level=INFO source=device.go:238 msg=\"total memory\" size=\"1.8 GiB\"\n",
      "time=2025-10-20T20:44:02.544Z level=INFO source=sched.go:482 msg=\"loaded runners\" count=2\n",
      "time=2025-10-20T20:44:02.544Z level=INFO source=server.go:1272 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-10-20T20:44:02.543Z level=INFO source=runner.go:1205 msg=load request=\"{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:2 GPULayers:29[ID:GPU-1df0b5e8-ab5e-0f0b-4f4f-216233b7cc9e Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-10-20T20:44:02.544Z level=INFO source=ggml.go:480 msg=\"offloading 28 repeating layers to GPU\"\n",
      "time=2025-10-20T20:44:02.544Z level=INFO source=ggml.go:487 msg=\"offloading output layer to GPU\"\n",
      "time=2025-10-20T20:44:02.544Z level=INFO source=ggml.go:492 msg=\"offloaded 29/29 layers to GPU\"\n",
      "time=2025-10-20T20:44:02.547Z level=INFO source=server.go:1306 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "time=2025-10-20T20:44:03.050Z level=INFO source=server.go:1310 msg=\"llama runner started in 1.02 seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author of *The Foundations of Science* is Henri Poincaré. Josiah Royce wrote the introduction, but the main author of the book is Poincaré. The work includes translations of \"Science and Hypothesis,\" \"The Value of Science,\" and \"Science and Method.\"\n",
      "[GIN] 2025/10/20 - 20:44:05 | 200 |  3.624498158s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    }
   ],
   "source": [
    "query = \"Who is the author The Foundations of Science?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T20:47:59.087650Z",
     "iopub.status.busy": "2025-10-20T20:47:59.087059Z",
     "iopub.status.idle": "2025-10-20T20:47:59.903541Z",
     "shell.execute_reply": "2025-10-20T20:47:59.902870Z",
     "shell.execute_reply.started": "2025-10-20T20:47:59.087630Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/10/20 - 20:47:59 | 200 |  152.733637ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "=== Top 10 from FAISS ===\n",
      "1: Author: Henri Poincaré\n",
      "\n",
      "Author of introduction, etc.: Josiah Royce\n",
      "\n",
      "Translator: George Bruce Halsted\n",
      "\n",
      "Release date: May 16, 2012 [eBook #39713]\n",
      "\n",
      "Language: English\n",
      "\n",
      "Credits: Produced by Bryan Ness and  ...\n",
      "2: ﻿The Project Gutenberg eBook of The Foundations of Science: Science and Hypothesis, The Value of Science, Science and Method\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      " ...\n",
      "3: Volume III. University Control. By J. MCKEEN CATTELL and other\n",
      "      authors.\n",
      "\n",
      "    AMERICAN MEN OF SCIENCE. A Biographical Directory.\n",
      "\n",
      "    SCIENCE. A weekly journal devoted to the advancement of scien ...\n",
      "4: A SERIES OF VOLUMES FOR THE PROMOTION OF\n",
      "  SCIENTIFIC RESEARCH AND EDUCATIONAL PROGRESS\n",
      "\n",
      "  EDITED BY J. MCKEEN CATTELL\n",
      "\n",
      "\n",
      "  VOLUME I--THE FOUNDATIONS OF SCIENCE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  UNDER THE SAME EDITORSHIP\n",
      "\n",
      "\n",
      "    SC ...\n",
      "5: Rados, \"permit me to make especial mention of his intensely interesting\n",
      "book, 'The Value of Science,' in which he in a way has laid down the\n",
      "scientist's creed.\" Now what is this creed? ...\n",
      "6: THE VALUE OF SCIENCE\n",
      "\n",
      "    Translator's Introduction                              201\n",
      "    Does the Scientist Create Science?                     201\n",
      "    The Mind Dispelling Optical Illusions            ...\n",
      "7: The branches of inquiry collectively known as the Philosophy of Science\n",
      "have undergone great changes since the appearance of Herbert Spencer's\n",
      "_First Principles_, that volume which a large part of the ...\n",
      "8: And above all the scientist must foresee. Carlyle has somewhere said\n",
      "something like this: \"Nothing but facts are of importance. John Lackland\n",
      "passed by here. Here is something that is admirable. Here  ...\n",
      "9: 3. _The Crude Fact and the Scientific Fact_\n",
      "\n",
      "What was most paradoxical in M. LeRoy's thesis was that affirmation that\n",
      "_the scientist creates the fact_; this was at the same time its\n",
      "essential point an ...\n",
      "10: Every age has ridiculed the one before it, and accused it of having\n",
      "generalized too quickly and too naïvely. Descartes pitied the Ionians;\n",
      "Descartes, in his turn, makes us smile. No doubt our children ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-10-20T20:47:59.141Z level=WARN source=types.go:753 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-10-20T20:47:59.141Z level=WARN source=types.go:753 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-10-20T20:47:59.141Z level=WARN source=types.go:753 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-10-20T20:47:59.141Z level=WARN source=types.go:753 msg=\"invalid option provided\" option=tfs_z\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5934daf9b67545d39d8fc268077e68b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Shortlist passed to Ollama ===\n",
      "1: Author: Henri Poincaré\n",
      "\n",
      "Author of introduction, etc.: Josiah Royce\n",
      "\n",
      "Translator: George Bruce Halsted\n",
      "\n",
      "Release date: May 16, 2012 [eBook #39713]\n",
      "\n",
      "Language: English\n",
      "\n",
      "Credits: Produced by Bryan Ness and  ...\n",
      "2: THE VALUE OF SCIENCE\n",
      "\n",
      "    Translator's Introduction                              201\n",
      "    Does the Scientist Create Science?                     201\n",
      "    The Mind Dispelling Optical Illusions            ...\n",
      "3: Every age has ridiculed the one before it, and accused it of having\n",
      "generalized too quickly and too naïvely. Descartes pitied the Ionians;\n",
      "Descartes, in his turn, makes us smile. No doubt our children ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 6976307200 total: 17179869184\n",
      "ggml_backend_cuda_device_get_memory utilizing NVML memory reporting free: 9018933248 total: 17179869184\n"
     ]
    }
   ],
   "source": [
    "# To check if reranking is working as expected\n",
    "\n",
    "base_docs = retriever.get_relevant_documents(\"Who is the author The Foundations of Science?\")\n",
    "print(\"=== Top 10 from FAISS ===\")\n",
    "for i, doc in enumerate(base_docs):\n",
    "    print(f\"{i+1}:\", doc.page_content[:200], \"...\")  # preview first 200 chars\n",
    "\n",
    "# Then rerank them using your reranker\n",
    "shortlist_docs = rerank_documents(\"Some question about the book\", base_docs, top_k=3)\n",
    "print(\"\\n=== Shortlist passed to Ollama ===\")\n",
    "for i, doc in enumerate(shortlist_docs):\n",
    "    print(f\"{i+1}:\", doc.page_content[:200], \"...\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8519266,
     "sourceId": 13422644,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
