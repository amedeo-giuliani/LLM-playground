{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a0e1dd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-18T13:00:28.240959Z",
     "iopub.status.busy": "2025-09-18T13:00:28.240696Z",
     "iopub.status.idle": "2025-09-18T13:00:44.705084Z",
     "shell.execute_reply": "2025-09-18T13:00:44.704158Z"
    },
    "papermill": {
     "duration": 16.470859,
     "end_time": "2025-09-18T13:00:44.706499",
     "exception": false,
     "start_time": "2025-09-18T13:00:28.235640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 13:00:31.729783: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758200431.909853      26 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758200431.963607      26 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/__script__.py\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/typing_extensions-4.15.0-py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/requests-2.32.5-py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/evaluate-0.4.5-py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/tqdm-4.67.1-py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/__results__.html\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/dill-0.4.0-py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/multiprocess-0.70.16-py311-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/pytz-2025.2-py2.py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/filelock-3.19.1-py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/idna-3.10-py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/urllib3-2.5.0-py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/aiohappyeyeballs-2.6.1-py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/input_requirements.txt\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/huggingface_hub-0.35.0-py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/aiosignal-1.4.0-py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/six-1.17.0-py2.py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/python_dateutil-2.9.0.post0-py2.py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/fsspec-2025.9.0-py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/attrs-25.3.0-py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/__script__.ipynb\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/packaging-25.0-py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/__output__.json\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/install_requirements.sh\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/certifi-2025.8.3-py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/tzdata-2025.2-py2.py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/datasets-4.1.0-py3-none-any.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-91260771-at-09-18-2025-12-59-26/custom.css\n",
      "/kaggle/input/llm-classification-finetuning/sample_submission.csv\n",
      "/kaggle/input/llm-classification-finetuning/train.csv\n",
      "/kaggle/input/llm-classification-finetuning/test.csv\n",
      "/kaggle/input/bert/keras/bert_small_en_uncased/2/config.json\n",
      "/kaggle/input/bert/keras/bert_small_en_uncased/2/tokenizer.json\n",
      "/kaggle/input/bert/keras/bert_small_en_uncased/2/metadata.json\n",
      "/kaggle/input/bert/keras/bert_small_en_uncased/2/model.weights.h5\n",
      "/kaggle/input/bert/keras/bert_small_en_uncased/2/assets/tokenizer/vocabulary.txt\n",
      "/kaggle/input/bert/keras/bert_small_en_uncased/3/config.json\n",
      "/kaggle/input/bert/keras/bert_small_en_uncased/3/tokenizer.json\n",
      "/kaggle/input/bert/keras/bert_small_en_uncased/3/metadata.json\n",
      "/kaggle/input/bert/keras/bert_small_en_uncased/3/model.weights.h5\n",
      "/kaggle/input/bert/keras/bert_small_en_uncased/3/assets/tokenizer/vocabulary.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import tensorflow as tf\n",
    "import keras_hub\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "# import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6db52b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:00:44.714656Z",
     "iopub.status.busy": "2025-09-18T13:00:44.714165Z",
     "iopub.status.idle": "2025-09-18T13:00:44.717891Z",
     "shell.execute_reply": "2025-09-18T13:00:44.717414Z"
    },
    "papermill": {
     "duration": 0.008601,
     "end_time": "2025-09-18T13:00:44.718891",
     "exception": false,
     "start_time": "2025-09-18T13:00:44.710290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert_small_en_uncased\"\n",
    "SEQUENCE_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2252453e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:00:44.725905Z",
     "iopub.status.busy": "2025-09-18T13:00:44.725701Z",
     "iopub.status.idle": "2025-09-18T13:00:48.257290Z",
     "shell.execute_reply": "2025-09-18T13:00:48.256412Z"
    },
    "papermill": {
     "duration": 3.536845,
     "end_time": "2025-09-18T13:00:48.258890",
     "exception": false,
     "start_time": "2025-09-18T13:00:44.722045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/train.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8482cba3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:00:48.267782Z",
     "iopub.status.busy": "2025-09-18T13:00:48.267511Z",
     "iopub.status.idle": "2025-09-18T13:00:48.296483Z",
     "shell.execute_reply": "2025-09-18T13:00:48.295829Z"
    },
    "papermill": {
     "duration": 0.03475,
     "end_time": "2025-09-18T13:00:48.297783",
     "exception": false,
     "start_time": "2025-09-18T13:00:48.263033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>[\"explain function calling. how would you call...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96401</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>[\"How can I create a test set for a very rare ...</td>\n",
       "      <td>[\"Creating a test set for a very rare category...</td>\n",
       "      <td>[\"When building a classifier for a very rare c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198779</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n",
       "      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n",
       "      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57472</th>\n",
       "      <td>4294656694</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>claude-1</td>\n",
       "      <td>[\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...</td>\n",
       "      <td>[\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...</td>\n",
       "      <td>[\"Here is how that mnemonic represents the dig...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57473</th>\n",
       "      <td>4294692063</td>\n",
       "      <td>claude-2.0</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>[\"In python, implement a naive Bayes with gaus...</td>\n",
       "      <td>[\"Here is an implementation of a naive Bayes c...</td>\n",
       "      <td>[\"Sure! Here's an implementation of a naive Ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57474</th>\n",
       "      <td>4294710549</td>\n",
       "      <td>claude-1</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>[\"is it unethical to work on building weapons?...</td>\n",
       "      <td>[\"Working on weapons technology raises some et...</td>\n",
       "      <td>[\"It depends on the context. Weapons can be us...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57475</th>\n",
       "      <td>4294899228</td>\n",
       "      <td>palm-2</td>\n",
       "      <td>tulu-2-dpo-70b</td>\n",
       "      <td>[\"If a bait contains 0,0025% bromadiolon then ...</td>\n",
       "      <td>[\"Bromadiolone is a rodenticide which is most ...</td>\n",
       "      <td>[\"As an AI language model, I do not promote or...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57476</th>\n",
       "      <td>4294947231</td>\n",
       "      <td>gemini-pro-dev-api</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>[\"three kids eat three apples in three days, h...</td>\n",
       "      <td>[\"27 apples\"]</td>\n",
       "      <td>[\"If three kids eat three apples in three days...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57477 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             model_a              model_b  \\\n",
       "0           30192  gpt-4-1106-preview           gpt-4-0613   \n",
       "1           53567           koala-13b           gpt-4-0613   \n",
       "2           65089  gpt-3.5-turbo-0613       mistral-medium   \n",
       "3           96401    llama-2-13b-chat  mistral-7b-instruct   \n",
       "4          198779           koala-13b   gpt-3.5-turbo-0314   \n",
       "...           ...                 ...                  ...   \n",
       "57472  4294656694          gpt-4-0613             claude-1   \n",
       "57473  4294692063          claude-2.0     llama-2-13b-chat   \n",
       "57474  4294710549            claude-1           alpaca-13b   \n",
       "57475  4294899228              palm-2       tulu-2-dpo-70b   \n",
       "57476  4294947231  gemini-pro-dev-api   gpt-4-1106-preview   \n",
       "\n",
       "                                                  prompt  \\\n",
       "0      [\"Is it morally right to try to have a certain...   \n",
       "1      [\"What is the difference between marriage lice...   \n",
       "2      [\"explain function calling. how would you call...   \n",
       "3      [\"How can I create a test set for a very rare ...   \n",
       "4      [\"What is the best way to travel from Tel-Aviv...   \n",
       "...                                                  ...   \n",
       "57472  [\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...   \n",
       "57473  [\"In python, implement a naive Bayes with gaus...   \n",
       "57474  [\"is it unethical to work on building weapons?...   \n",
       "57475  [\"If a bait contains 0,0025% bromadiolon then ...   \n",
       "57476  [\"three kids eat three apples in three days, h...   \n",
       "\n",
       "                                              response_a  \\\n",
       "0      [\"The question of whether it is morally right ...   \n",
       "1      [\"A marriage license is a legal document that ...   \n",
       "2      [\"Function calling is the process of invoking ...   \n",
       "3      [\"Creating a test set for a very rare category...   \n",
       "4      [\"The best way to travel from Tel Aviv to Jeru...   \n",
       "...                                                  ...   \n",
       "57472  [\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...   \n",
       "57473  [\"Here is an implementation of a naive Bayes c...   \n",
       "57474  [\"Working on weapons technology raises some et...   \n",
       "57475  [\"Bromadiolone is a rodenticide which is most ...   \n",
       "57476                                      [\"27 apples\"]   \n",
       "\n",
       "                                              response_b  winner_model_a  \\\n",
       "0      [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1      [\"A marriage license and a marriage certificat...               0   \n",
       "2      [\"Function calling is the process of invoking ...               0   \n",
       "3      [\"When building a classifier for a very rare c...               1   \n",
       "4      [\"The best way to travel from Tel-Aviv to Jeru...               0   \n",
       "...                                                  ...             ...   \n",
       "57472  [\"Here is how that mnemonic represents the dig...               1   \n",
       "57473  [\"Sure! Here's an implementation of a naive Ba...               1   \n",
       "57474  [\"It depends on the context. Weapons can be us...               1   \n",
       "57475  [\"As an AI language model, I do not promote or...               0   \n",
       "57476  [\"If three kids eat three apples in three days...               1   \n",
       "\n",
       "       winner_model_b  winner_tie  \n",
       "0                   0           0  \n",
       "1                   1           0  \n",
       "2                   0           1  \n",
       "3                   0           0  \n",
       "4                   1           0  \n",
       "...               ...         ...  \n",
       "57472               0           0  \n",
       "57473               0           0  \n",
       "57474               0           0  \n",
       "57475               1           0  \n",
       "57476               0           0  \n",
       "\n",
       "[57477 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bb3f2b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:00:48.307040Z",
     "iopub.status.busy": "2025-09-18T13:00:48.306782Z",
     "iopub.status.idle": "2025-09-18T13:00:51.898793Z",
     "shell.execute_reply": "2025-09-18T13:00:51.898148Z"
    },
    "papermill": {
     "duration": 3.598215,
     "end_time": "2025-09-18T13:00:51.900202",
     "exception": false,
     "start_time": "2025-09-18T13:00:48.301987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Take the first prompt and its associated response\n",
    "train_df[\"prompt\"] = train_df.prompt.map(lambda x: eval(x)[0])\n",
    "train_df[\"response_a\"] = train_df.response_a.map(lambda x: eval(x.replace(\"null\",\"''\"))[0])\n",
    "train_df[\"response_b\"] = train_df.response_b.map(lambda x: eval(x.replace(\"null\", \"''\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1228d4cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:00:51.909138Z",
     "iopub.status.busy": "2025-09-18T13:00:51.908670Z",
     "iopub.status.idle": "2025-09-18T13:00:51.920262Z",
     "shell.execute_reply": "2025-09-18T13:00:51.919438Z"
    },
    "papermill": {
     "duration": 0.017225,
     "end_time": "2025-09-18T13:00:51.921462",
     "exception": false,
     "start_time": "2025-09-18T13:00:51.904237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>Is it morally right to try to have a certain p...</td>\n",
       "      <td>The question of whether it is morally right to...</td>\n",
       "      <td>As an AI, I don't have personal beliefs or opi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53567</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>What is the difference between marriage licens...</td>\n",
       "      <td>A marriage license is a legal document that al...</td>\n",
       "      <td>A marriage license and a marriage certificate ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65089</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>explain function calling. how would you call a...</td>\n",
       "      <td>Function calling is the process of invoking or...</td>\n",
       "      <td>Function calling is the process of invoking a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96401</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>How can I create a test set for a very rare ca...</td>\n",
       "      <td>Creating a test set for a very rare category c...</td>\n",
       "      <td>When building a classifier for a very rare cat...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198779</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>What is the best way to travel from Tel-Aviv t...</td>\n",
       "      <td>The best way to travel from Tel Aviv to Jerusa...</td>\n",
       "      <td>The best way to travel from Tel-Aviv to Jerusa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57472</th>\n",
       "      <td>4294656694</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>claude-1</td>\n",
       "      <td>A simple mnemonic for π:\\n\"How I wish I could ...</td>\n",
       "      <td>Sure, let's break it down:\\n\\n1. \"How\" has 3 l...</td>\n",
       "      <td>Here is how that mnemonic represents the digit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57473</th>\n",
       "      <td>4294692063</td>\n",
       "      <td>claude-2.0</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>In python, implement a naive Bayes with gaussi...</td>\n",
       "      <td>Here is an implementation of a naive Bayes cla...</td>\n",
       "      <td>Sure! Here's an implementation of a naive Baye...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57474</th>\n",
       "      <td>4294710549</td>\n",
       "      <td>claude-1</td>\n",
       "      <td>alpaca-13b</td>\n",
       "      <td>is it unethical to work on building weapons?</td>\n",
       "      <td>Working on weapons technology raises some ethi...</td>\n",
       "      <td>It depends on the context. Weapons can be used...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57475</th>\n",
       "      <td>4294899228</td>\n",
       "      <td>palm-2</td>\n",
       "      <td>tulu-2-dpo-70b</td>\n",
       "      <td>If a bait contains 0,0025% bromadiolon then ho...</td>\n",
       "      <td>Bromadiolone is a rodenticide which is most of...</td>\n",
       "      <td>As an AI language model, I do not promote or c...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57476</th>\n",
       "      <td>4294947231</td>\n",
       "      <td>gemini-pro-dev-api</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>three kids eat three apples in three days, how...</td>\n",
       "      <td>27 apples</td>\n",
       "      <td>If three kids eat three apples in three days, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57477 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             model_a              model_b  \\\n",
       "0           30192  gpt-4-1106-preview           gpt-4-0613   \n",
       "1           53567           koala-13b           gpt-4-0613   \n",
       "2           65089  gpt-3.5-turbo-0613       mistral-medium   \n",
       "3           96401    llama-2-13b-chat  mistral-7b-instruct   \n",
       "4          198779           koala-13b   gpt-3.5-turbo-0314   \n",
       "...           ...                 ...                  ...   \n",
       "57472  4294656694          gpt-4-0613             claude-1   \n",
       "57473  4294692063          claude-2.0     llama-2-13b-chat   \n",
       "57474  4294710549            claude-1           alpaca-13b   \n",
       "57475  4294899228              palm-2       tulu-2-dpo-70b   \n",
       "57476  4294947231  gemini-pro-dev-api   gpt-4-1106-preview   \n",
       "\n",
       "                                                  prompt  \\\n",
       "0      Is it morally right to try to have a certain p...   \n",
       "1      What is the difference between marriage licens...   \n",
       "2      explain function calling. how would you call a...   \n",
       "3      How can I create a test set for a very rare ca...   \n",
       "4      What is the best way to travel from Tel-Aviv t...   \n",
       "...                                                  ...   \n",
       "57472  A simple mnemonic for π:\\n\"How I wish I could ...   \n",
       "57473  In python, implement a naive Bayes with gaussi...   \n",
       "57474       is it unethical to work on building weapons?   \n",
       "57475  If a bait contains 0,0025% bromadiolon then ho...   \n",
       "57476  three kids eat three apples in three days, how...   \n",
       "\n",
       "                                              response_a  \\\n",
       "0      The question of whether it is morally right to...   \n",
       "1      A marriage license is a legal document that al...   \n",
       "2      Function calling is the process of invoking or...   \n",
       "3      Creating a test set for a very rare category c...   \n",
       "4      The best way to travel from Tel Aviv to Jerusa...   \n",
       "...                                                  ...   \n",
       "57472  Sure, let's break it down:\\n\\n1. \"How\" has 3 l...   \n",
       "57473  Here is an implementation of a naive Bayes cla...   \n",
       "57474  Working on weapons technology raises some ethi...   \n",
       "57475  Bromadiolone is a rodenticide which is most of...   \n",
       "57476                                          27 apples   \n",
       "\n",
       "                                              response_b  winner_model_a  \\\n",
       "0      As an AI, I don't have personal beliefs or opi...               1   \n",
       "1      A marriage license and a marriage certificate ...               0   \n",
       "2      Function calling is the process of invoking a ...               0   \n",
       "3      When building a classifier for a very rare cat...               1   \n",
       "4      The best way to travel from Tel-Aviv to Jerusa...               0   \n",
       "...                                                  ...             ...   \n",
       "57472  Here is how that mnemonic represents the digit...               1   \n",
       "57473  Sure! Here's an implementation of a naive Baye...               1   \n",
       "57474  It depends on the context. Weapons can be used...               1   \n",
       "57475  As an AI language model, I do not promote or c...               0   \n",
       "57476  If three kids eat three apples in three days, ...               1   \n",
       "\n",
       "       winner_model_b  winner_tie  \n",
       "0                   0           0  \n",
       "1                   1           0  \n",
       "2                   0           1  \n",
       "3                   0           0  \n",
       "4                   1           0  \n",
       "...               ...         ...  \n",
       "57472               0           0  \n",
       "57473               0           0  \n",
       "57474               0           0  \n",
       "57475               1           0  \n",
       "57476               0           0  \n",
       "\n",
       "[57477 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8bb09dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:00:51.931788Z",
     "iopub.status.busy": "2025-09-18T13:00:51.931210Z",
     "iopub.status.idle": "2025-09-18T13:00:54.004348Z",
     "shell.execute_reply": "2025-09-18T13:00:54.003361Z"
    },
    "papermill": {
     "duration": 2.079435,
     "end_time": "2025-09-18T13:00:54.005682",
     "exception": false,
     "start_time": "2025-09-18T13:00:51.926247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758200453.935370      26 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1758200453.936072      26 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = keras_hub.tokenizers.BertTokenizer.from_preset(MODEL_NAME)\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = str(text)\n",
    "        \n",
    "        # Handle literal \\n characters (escaped newlines in the text)\n",
    "        text = text.replace('\\\\n', ' ')\n",
    "        text = text.replace('\\\\r', ' ')\n",
    "        text = text.replace('\\\\t', ' ')\n",
    "        \n",
    "        ### Light\n",
    "        text = text.lower() # lowercase everything\n",
    "        text = re.sub(r\"\\s+\", \" \", text)  # Collapse whitespace\n",
    "        text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)  # Remove non-ASCII\n",
    "    \n",
    "        text = text.replace('\\\\', ' ')\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    def create_input_pairs(self, row):\n",
    "        prompt = self.clean_text(row['prompt'])\n",
    "        return [\n",
    "            f\"Prompt: {prompt} {self.tokenizer.sep_token} Response: {self.clean_text(row['response_a'])}\",\n",
    "            f\"Prompt: {prompt} {self.tokenizer.sep_token} Response: {self.clean_text(row['response_b'])}\"\n",
    "        ]\n",
    "\n",
    "processor = TextPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9136dfae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:00:54.014903Z",
     "iopub.status.busy": "2025-09-18T13:00:54.014656Z",
     "iopub.status.idle": "2025-09-18T13:01:05.467581Z",
     "shell.execute_reply": "2025-09-18T13:01:05.466747Z"
    },
    "papermill": {
     "duration": 11.459025,
     "end_time": "2025-09-18T13:01:05.469080",
     "exception": false,
     "start_time": "2025-09-18T13:00:54.010055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Prompt: is it morally right to try to have a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Prompt: what is the difference between marria...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Prompt: explain function calling. how would y...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Prompt: how can i create a test set for a ver...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Prompt: what is the best way to travel from t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  label\n",
       "0  [Prompt: is it morally right to try to have a ...      0\n",
       "1  [Prompt: what is the difference between marria...      1\n",
       "2  [Prompt: explain function calling. how would y...      2\n",
       "3  [Prompt: how can i create a test set for a ver...      0\n",
       "4  [Prompt: what is the best way to travel from t...      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply preprocessing\n",
    "train_df['inputs'] = train_df.apply(processor.create_input_pairs, axis=1)\n",
    "test_df['inputs'] = test_df.apply(processor.create_input_pairs, axis=1)\n",
    "\n",
    "# Create labels (0: model_a wins, 1: model_b wins, 2: tie)\n",
    "train_df['label'] = train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1)\n",
    "train_df['label'] = train_df['label'].map({'winner_model_a': 0, 'winner_model_b': 1, 'winner_tie': 2})\n",
    "\n",
    "# Preview processed data\n",
    "train_df[['inputs', 'label']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a641f847",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:05.478537Z",
     "iopub.status.busy": "2025-09-18T13:01:05.478292Z",
     "iopub.status.idle": "2025-09-18T13:01:05.485169Z",
     "shell.execute_reply": "2025-09-18T13:01:05.484383Z"
    },
    "papermill": {
     "duration": 0.013023,
     "end_time": "2025-09-18T13:01:05.486620",
     "exception": false,
     "start_time": "2025-09-18T13:01:05.473597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Prompt: if a bait contains 0,0025% bromadiolon then how much an adult rat needs to consume to die within 24 hours? [SEP] Response: bromadiolone is a rodenticide which is most often used in bait. it is a second generation anticoagulant that blocks the production of vitamin k1, which is essential for blood clotting. without vitamin k1, the blood will not clot properly and the rat will bleed to death. bromadiolone is effective at killing rats in a matter of hours and is often used in areas where rats are a problem, such as around homes, businesses, and farms.',\n",
       " 'Prompt: if a bait contains 0,0025% bromadiolon then how much an adult rat needs to consume to die within 24 hours? [SEP] Response: as an ai language model, i do not promote or condone the use of poison or harm to any living beings. however, i can provide you with some information based on the question you asked. bromadiolone is a highly toxic rodenticide, and the lethal dose required to kill an adult rat can vary depending on several factors, such as the size and weight of the rat, its age, and its overall health. according to some sources, the ld50 (median lethal dose) for bromadiolone in rats is approximately 1-5 mg /kg of body weight. this means that a rat weighing 250 grams (0.25 kg) would need to consume approximately 2.5 to 12.5 milligrams of bromadiolone to have a 50% chance of dying within 24 hours. however, please note that the use of lethal rodenticides can have unintended consequences and may harm non-target animals, including pets and wildlife. it is important to consider alternative methods of rodent control that are more humane and environmentally friendly.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['inputs', 'label']].iloc[57475][\"inputs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bb49a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:05.498537Z",
     "iopub.status.busy": "2025-09-18T13:01:05.498300Z",
     "iopub.status.idle": "2025-09-18T13:01:05.542754Z",
     "shell.execute_reply": "2025-09-18T13:01:05.541929Z"
    },
    "papermill": {
     "duration": 0.050739,
     "end_time": "2025-09-18T13:01:05.544262",
     "exception": false,
     "start_time": "2025-09-18T13:01:05.493523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    train_df, \n",
    "    test_size=0.1, \n",
    "    stratify=train_df['label'],\n",
    "    random_state=1025\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "326054f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:05.554459Z",
     "iopub.status.busy": "2025-09-18T13:01:05.554004Z",
     "iopub.status.idle": "2025-09-18T13:01:05.560179Z",
     "shell.execute_reply": "2025-09-18T13:01:05.559566Z"
    },
    "papermill": {
     "duration": 0.012243,
     "end_time": "2025-09-18T13:01:05.561323",
     "exception": false,
     "start_time": "2025-09-18T13:01:05.549080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_pair(text_pair, label=None):\n",
    "    \"\"\"Convert raw text pairs to model-ready format\"\"\"\n",
    "    # Tokenize each response separately\n",
    "    processed_a = preprocessor(text_pair[0])\n",
    "    processed_b = preprocessor(text_pair[1])\n",
    "\n",
    "    # Stack to create (2, seq_len) tensors\n",
    "    model_inputs = {\n",
    "        \"token_ids\": tf.stack([processed_a[\"token_ids\"], processed_b[\"token_ids\"]], axis=0),\n",
    "        \"padding_mask\": tf.stack([processed_a[\"padding_mask\"], processed_b[\"padding_mask\"]], axis=0),\n",
    "        \"segment_ids\": tf.stack([processed_a[\"segment_ids\"], processed_b[\"segment_ids\"]], axis=0)\n",
    "    }\n",
    "    return (model_inputs, label) if label is not None else model_inputs\n",
    "\n",
    "def create_dataset(text_pairs, labels=None, preprocessor=None):\n",
    "    \"\"\"Convert to optimized TF Dataset with proper input pair handling\"\"\"\n",
    "    AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "    # Convert to TensorFlow Dataset\n",
    "    if labels is not None:\n",
    "        ds = tf.data.Dataset.from_tensor_slices((text_pairs, labels))\n",
    "        ds = ds.shuffle(1025)\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_tensor_slices(text_pairs)\n",
    "\n",
    "    # Apply preprocessing and batching\n",
    "    ds = ds.map(preprocess_pair, num_parallel_calls=AUTO)\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caa754e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:05.570903Z",
     "iopub.status.busy": "2025-09-18T13:01:05.570319Z",
     "iopub.status.idle": "2025-09-18T13:01:06.726601Z",
     "shell.execute_reply": "2025-09-18T13:01:06.725983Z"
    },
    "papermill": {
     "duration": 1.162455,
     "end_time": "2025-09-18T13:01:06.727965",
     "exception": false,
     "start_time": "2025-09-18T13:01:05.565510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = keras_hub.models.TextClassifierPreprocessor.from_preset(\n",
    "    MODEL_NAME,\n",
    "    sequence_length=SEQUENCE_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e8c305c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:06.738160Z",
     "iopub.status.busy": "2025-09-18T13:01:06.737870Z",
     "iopub.status.idle": "2025-09-18T13:01:16.724280Z",
     "shell.execute_reply": "2025-09-18T13:01:16.723464Z"
    },
    "papermill": {
     "duration": 9.993067,
     "end_time": "2025-09-18T13:01:16.725665",
     "exception": false,
     "start_time": "2025-09-18T13:01:06.732598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare all datasets\n",
    "train_ds = create_dataset(\n",
    "    train_df['inputs'].tolist(), \n",
    "    tf.keras.utils.to_categorical(train_df['label']),\n",
    "    preprocessor=preprocessor\n",
    ")\n",
    "val_ds = create_dataset(\n",
    "    val_df['inputs'].tolist(), \n",
    "    tf.keras.utils.to_categorical(val_df['label']),\n",
    "    preprocessor=preprocessor\n",
    ")\n",
    "test_ds = create_dataset(\n",
    "    test_df['inputs'].tolist(),\n",
    "    preprocessor=preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d922bb86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:16.735606Z",
     "iopub.status.busy": "2025-09-18T13:01:16.734952Z",
     "iopub.status.idle": "2025-09-18T13:01:17.481188Z",
     "shell.execute_reply": "2025-09-18T13:01:17.480340Z"
    },
    "papermill": {
     "duration": 0.752275,
     "end_time": "2025-09-18T13:01:17.482440",
     "exception": false,
     "start_time": "2025-09-18T13:01:16.730165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs shape: (16, 2, 128)\n",
      "Mask shape: (16, 2, 128)\n",
      "Segment IDs shape: (16, 2, 128)\n",
      "Example token_ids[0,0,:5]: tf.Tensor([  101 25732  1024  1045  2052], shape=(5,), dtype=int32) tf.Tensor([0 0 0 0 0], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    inputs, labels = batch\n",
    "    print(\"Token IDs shape:\", inputs[\"token_ids\"].shape)  # Should be (batch_size, 2, 128)\n",
    "    print(\"Mask shape:\", inputs[\"padding_mask\"].shape)\n",
    "    print(\"Segment IDs shape:\", inputs[\"segment_ids\"].shape)\n",
    "    print(\"Example token_ids[0,0,:5]:\", inputs[\"token_ids\"][0,0,:5], inputs[\"segment_ids\"][0,0,:5])  # First 5 tokens of response_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "522c72e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:17.492186Z",
     "iopub.status.busy": "2025-09-18T13:01:17.491924Z",
     "iopub.status.idle": "2025-09-18T13:01:21.465592Z",
     "shell.execute_reply": "2025-09-18T13:01:21.465023Z"
    },
    "papermill": {
     "duration": 3.979993,
     "end_time": "2025-09-18T13:01:21.466866",
     "exception": false,
     "start_time": "2025-09-18T13:01:17.486873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_response(inputs, index):\n",
    "    return {\n",
    "        \"token_ids\": inputs[\"token_ids\"][:, index, :],\n",
    "        \"padding_mask\": inputs[\"padding_mask\"][:, index, :],\n",
    "        \"segment_ids\": inputs[\"segment_ids\"][:, index, :]\n",
    "    }\n",
    "    \n",
    "def build_bert_classifier():\n",
    "    inputs = {\n",
    "        \"token_ids\": tf.keras.Input(shape=(2, SEQUENCE_LENGTH), dtype=tf.int32, name=\"token_ids\"),\n",
    "        \"padding_mask\": tf.keras.Input(shape=(2, SEQUENCE_LENGTH), dtype=tf.int32, name=\"padding_mask\"),\n",
    "        \"segment_ids\": tf.keras.Input(shape=(2, SEQUENCE_LENGTH), dtype=tf.int32, name=\"segment_ids\")\n",
    "    }\n",
    "\n",
    "    backbone = keras_hub.models.BertBackbone.from_preset(MODEL_NAME)\n",
    "\n",
    "    emb_a = backbone(process_response(inputs, 0))[\"pooled_output\"]\n",
    "    emb_b = backbone(process_response(inputs, 1))[\"pooled_output\"]\n",
    "\n",
    "    embs = tf.keras.layers.Concatenate()([emb_a, emb_b])\n",
    "    x = embs\n",
    "    #x = tf.keras.layers.GlobalAveragePooling1D()(embs)\n",
    "    #x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    outputs = tf.keras.layers.Dense(3, activation=\"softmax\", name=\"classifier\")(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model = build_bert_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f4fe2e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:21.478198Z",
     "iopub.status.busy": "2025-09-18T13:01:21.477937Z",
     "iopub.status.idle": "2025-09-18T13:01:21.499128Z",
     "shell.execute_reply": "2025-09-18T13:01:21.498539Z"
    },
    "papermill": {
     "duration": 0.028722,
     "end_time": "2025-09-18T13:01:21.500306",
     "exception": false,
     "start_time": "2025-09-18T13:01:21.471584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ segment_ids         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ token_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_2          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_3          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ segment_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_5          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_6          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ segment_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_4          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bert_backbone       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">28,763,648</span> │ get_item_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertBackbone</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)] │            │ get_item_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ get_item_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ get_item_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ get_item_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ get_item_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bert_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ bert_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ classifier (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,075</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ segment_ids         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ token_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_2          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mGetItem\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_3          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ segment_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mGetItem\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mGetItem\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_5          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mGetItem\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_6          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ segment_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mGetItem\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_4          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mGetItem\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bert_backbone       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),     │ \u001b[38;5;34m28,763,648\u001b[0m │ get_item_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mBertBackbone\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m)] │            │ get_item_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ get_item_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ get_item_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ get_item_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ get_item_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ bert_backbone[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ bert_backbone[\u001b[38;5;34m1\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ classifier (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │      \u001b[38;5;34m3,075\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,766,723</span> (109.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m28,766,723\u001b[0m (109.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,766,723</span> (109.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m28,766,723\u001b[0m (109.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "816ae2f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:21.511961Z",
     "iopub.status.busy": "2025-09-18T13:01:21.511360Z",
     "iopub.status.idle": "2025-09-18T13:01:21.525723Z",
     "shell.execute_reply": "2025-09-18T13:01:21.524966Z"
    },
    "papermill": {
     "duration": 0.021497,
     "end_time": "2025-09-18T13:01:21.526995",
     "exception": false,
     "start_time": "2025-09-18T13:01:21.505498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(LEARNING_RATE, weight_decay=0.01),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=[\"accuracy\", \"categorical_crossentropy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "247f04f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:21.538060Z",
     "iopub.status.busy": "2025-09-18T13:01:21.537821Z",
     "iopub.status.idle": "2025-09-18T13:47:39.693878Z",
     "shell.execute_reply": "2025-09-18T13:47:39.693142Z"
    },
    "papermill": {
     "duration": 2778.710726,
     "end_time": "2025-09-18T13:47:40.242955",
     "exception": false,
     "start_time": "2025-09-18T13:01:21.532229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1758200508.323025      69 service.cc:148] XLA service 0x7d885800c630 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1758200508.324454      69 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1758200508.324479      69 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1758200510.905935      69 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1758200532.502617      69 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3234/3234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m615s\u001b[0m 174ms/step - accuracy: 0.3724 - categorical_crossentropy: 1.1094 - loss: 1.1121 - val_accuracy: 0.4410 - val_categorical_crossentropy: 1.0594 - val_loss: 1.0676 - learning_rate: 5.0000e-06\n",
      "Epoch 2/5\n",
      "\u001b[1m3234/3234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m542s\u001b[0m 167ms/step - accuracy: 0.4264 - categorical_crossentropy: 1.0641 - loss: 1.0714 - val_accuracy: 0.4476 - val_categorical_crossentropy: 1.0521 - val_loss: 1.0624 - learning_rate: 5.0000e-06\n",
      "Epoch 3/5\n",
      "\u001b[1m3234/3234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m541s\u001b[0m 167ms/step - accuracy: 0.4501 - categorical_crossentropy: 1.0474 - loss: 1.0575 - val_accuracy: 0.4553 - val_categorical_crossentropy: 1.0502 - val_loss: 1.0617 - learning_rate: 5.0000e-06\n",
      "Epoch 4/5\n",
      "\u001b[1m3234/3234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m540s\u001b[0m 167ms/step - accuracy: 0.4709 - categorical_crossentropy: 1.0321 - loss: 1.0448 - val_accuracy: 0.4483 - val_categorical_crossentropy: 1.0543 - val_loss: 1.0671 - learning_rate: 5.0000e-06\n",
      "Epoch 5/5\n",
      "\u001b[1m3234/3234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m540s\u001b[0m 167ms/step - accuracy: 0.4923 - categorical_crossentropy: 1.0088 - loss: 1.0252 - val_accuracy: 0.4492 - val_categorical_crossentropy: 1.0556 - val_loss: 1.0694 - learning_rate: 2.5000e-06\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"best_model.weights.h5\", save_best_only=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=1)\n",
    "]\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cfb559d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:47:41.732235Z",
     "iopub.status.busy": "2025-09-18T13:47:41.731942Z",
     "iopub.status.idle": "2025-09-18T13:47:46.634584Z",
     "shell.execute_reply": "2025-09-18T13:47:46.633937Z"
    },
    "papermill": {
     "duration": 5.609397,
     "end_time": "2025-09-18T13:47:46.635886",
     "exception": false,
     "start_time": "2025-09-18T13:47:41.026489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n"
     ]
    }
   ],
   "source": [
    "probs = model.predict(test_ds)\n",
    "preds = np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1114e8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:47:48.104015Z",
     "iopub.status.busy": "2025-09-18T13:47:48.103738Z",
     "iopub.status.idle": "2025-09-18T13:47:48.118499Z",
     "shell.execute_reply": "2025-09-18T13:47:48.117872Z"
    },
    "papermill": {
     "duration": 0.714548,
     "end_time": "2025-09-18T13:47:48.119568",
     "exception": false,
     "start_time": "2025-09-18T13:47:47.405020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>0.320571</td>\n",
       "      <td>0.467139</td>\n",
       "      <td>0.212290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>0.307244</td>\n",
       "      <td>0.376846</td>\n",
       "      <td>0.315910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>0.357008</td>\n",
       "      <td>0.352556</td>\n",
       "      <td>0.290437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  winner_model_a  winner_model_b  winner_tie\n",
       "0   136060        0.320571        0.467139    0.212290\n",
       "1   211333        0.307244        0.376846    0.315910\n",
       "2  1233961        0.357008        0.352556    0.290437"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame({'id': test_df[\"id\"],\n",
    "                        'winner_model_a': probs[:, 0],\n",
    "                        'winner_model_b': probs[:, 1],\n",
    "                        'winner_tie': probs[:, 2]})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9809560,
     "sourceId": 86518,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 10555382,
     "modelInstanceId": 4676,
     "sourceId": 204970,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 7429175,
     "modelInstanceId": 4676,
     "sourceId": 6055,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2847.682303,
   "end_time": "2025-09-18T13:47:52.047188",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-18T13:00:24.364885",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
